{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNIjFGXNlKtzA+2N0LZsSU/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Bharath-tars/House_prices_Supervised_model/blob/main/House_me.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Importing all the essential libraries required for solving the problem statement"
      ],
      "metadata": {
        "id": "uxDH83sxlj6M"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oZgZSzQZNVTJ"
      },
      "outputs": [],
      "source": [
        "import copy, math\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Importing the dataset from the filepath and dividing it into features and target values\n",
        "\n",
        "**point to remember**\n",
        "->The filepath of the dataset should we same as source code file otherwise change the file path according to the directories."
      ],
      "metadata": {
        "id": "cyXbvFinluHx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = np.loadtxt(\"vertopal.com_houses (1).ipynb\", delimiter=',')\n",
        "X_train = data[:,:4]\n",
        "y_train = data[:,4:]\n",
        "X_features = ['size(sqft)','bedrooms','floors','age']"
      ],
      "metadata": {
        "id": "JA7JpHpxPN4X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Plotting the dataset values inform of graphs to get an precise look into the analysis"
      ],
      "metadata": {
        "id": "AzGCDtqlmNv0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fig,ax=plt.subplots(1, 4, figsize=(12, 3), sharey=True)\n",
        "for i in range(len(ax)):\n",
        "    ax[i].scatter(X_train[:,i],y_train)\n",
        "    ax[i].set_xlabel(X_features[i])\n",
        "ax[0].set_ylabel(\"Price (1000's)\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "qcuJBCiiQhhl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Optional** -- This is just for just knowing the info about the training set."
      ],
      "metadata": {
        "id": "4NzUCH_hmbKZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"X Shape: {X_train.shape}, X Type:{type(X_train)})\")\n",
        "print(X_train)\n",
        "print(f\"y Shape: {y_train.shape}, y Type:{type(y_train)})\")\n",
        "print(y_train)"
      ],
      "metadata": {
        "id": "9zGDf709k297"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Z-normalization function which normalizes the features into readable and easy calculated format.. it includes numpy functions like mean and standard deviation"
      ],
      "metadata": {
        "id": "RrPIwM6wmzm6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def zscore_normalize_features(X):\n",
        "    mu     = np.mean(X, axis=0)                 \n",
        "    sigma  = np.std(X, axis=0)                  \n",
        "    X_norm = (X - mu) / sigma      \n",
        "    return (X_norm, mu, sigma)"
      ],
      "metadata": {
        "id": "TnDajsJZk3GJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is the initialization of the zscore function "
      ],
      "metadata": {
        "id": "rDyhJgXpnL7c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_norm, X_mu, X_sigma = zscore_normalize_features(X_train)\n",
        "print(f\"X_mu = {X_mu}, \\nX_sigma = {X_sigma}\")\n",
        "print(f\"Peak to Peak range by column in Raw        X:{np.ptp(X_train,axis=0)}\")   \n",
        "print(f\"Peak to Peak range by column in Normalized X:{np.ptp(X_norm,axis=0)}\")"
      ],
      "metadata": {
        "id": "r4o6EG9kk3PE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Basically when we are working with supervised learning algorithms especially linear regresion we use bias(b) and coeffcient(w) as parameters to make our ml model more powerful"
      ],
      "metadata": {
        "id": "FYhyUav4nTBq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "b_init = 785.1811367994083\n",
        "w_init = np.array([ 0.39133535, 18.75376741, -53.36032453, -26.42131618])\n",
        "print(f\"w_init shape: {w_init.shape}, b_init type: {type(b_init)}\")"
      ],
      "metadata": {
        "id": "3ABNlRqyk3Qb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is the linear regression model function"
      ],
      "metadata": {
        "id": "OCnSUJlpnmvB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(x, w, b): \n",
        "    p = np.dot(x, w) + b     \n",
        "    return p    "
      ],
      "metadata": {
        "id": "-9KG-0CDk3S_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Shape of the training set"
      ],
      "metadata": {
        "id": "5k4tbGSmnvY7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x_vec = X_train[0,:]\n",
        "print(f\"x_vec shape {x_vec.shape}, x_vec value: {x_vec}\")\n",
        "\n",
        "f_wb = predict(x_vec,w_init, b_init)\n",
        "print(f\"f_wb shape {f_wb.shape}, prediction: {f_wb}\")"
      ],
      "metadata": {
        "id": "I_Bi5FFRk3Vs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is the cost function which is gonna return the differnece between the target and prediction which is ultimate factor which determines the quality and performance of our algorithm"
      ],
      "metadata": {
        "id": "vOqyV2Zcn3J-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_cost(X, y, w, b): \n",
        "    m = X.shape[0]\n",
        "    cost = 0.0\n",
        "    for i in range(m):                                \n",
        "        f_wb_i = np.dot(X[i], w) + b           \n",
        "        cost = cost + (f_wb_i - y[i])**2       \n",
        "    cost = cost / (2 * m)                       \n",
        "    return cost"
      ],
      "metadata": {
        "id": "CYJl5qvxlBXu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Initialization of the cost function"
      ],
      "metadata": {
        "id": "Pk0xEDwvoKEO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cost = compute_cost(X_train, y_train, w_init, b_init)\n",
        "print(f'Cost at optimal w : {cost}')"
      ],
      "metadata": {
        "id": "Gi-YqmCVlBat"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is the function that returns the partial derivatives of both the parameters that are gonna be used in gradient desent for predicting the values of w and b"
      ],
      "metadata": {
        "id": "59bW4bEeoOnu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_gradient(X, y, w, b): \n",
        "    \n",
        "    m,n = X.shape           #(number of examples, number of features)\n",
        "    dj_dw = np.zeros((n,))\n",
        "    dj_db = 0.\n",
        "\n",
        "    for i in range(m):                             \n",
        "        err = (np.dot(X[i], w) + b) - y[i]   \n",
        "        for j in range(n):                         \n",
        "            dj_dw[j] = dj_dw[j] + err * X[i, j]    \n",
        "        dj_db = dj_db + err                        \n",
        "    dj_dw = dj_dw / m                                \n",
        "    dj_db = dj_db / m                                \n",
        "        \n",
        "    return dj_db, dj_dw"
      ],
      "metadata": {
        "id": "3_K9d6oJlE4B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Initialization of partial derivatives"
      ],
      "metadata": {
        "id": "ZbzLGySKokGv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tmp_dj_db, tmp_dj_dw = compute_gradient(X_train, y_train, w_init, b_init)\n",
        "print(f'dj_db at initial w,b: {tmp_dj_db}')\n",
        "print(f'dj_dw at initial w,b: \\n {tmp_dj_dw}')"
      ],
      "metadata": {
        "id": "fimI3ofZlE_H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is the gradient desent that is automatically compute the values of w and b which is used for minimizing the value of cost function"
      ],
      "metadata": {
        "id": "75G7whldorGo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def gradient_descent(X, y, w_in, b_in, cost_function, gradient_function, alpha, num_iters): \n",
        "    \n",
        "    J_history = []\n",
        "    w = copy.deepcopy(w_in)  #avoids modifying\n",
        "    b = b_in\n",
        "    \n",
        "    for i in range(num_iters):\n",
        "\n",
        "        dj_db,dj_dw = gradient_function(X, y, w, b)   \n",
        "        \n",
        "        w = w - alpha * dj_dw               \n",
        "        b = b - alpha * dj_db               \n",
        "      \n",
        "        # Saves cost J at each iteration\n",
        "        if i<100000:      # prevent resource exhaustion \n",
        "            J_history.append( cost_function(X, y, w, b))\n",
        "\n",
        "        # # Prints cost every at intervals 10 times or as many iterations if < 10\n",
        "        # if i% math.ceil(num_iters / 10) == 0:\n",
        "        #     print(f\"Iteration {i:4d}: Cost {J_history[-1]:8.2f}   \")\n",
        "        \n",
        "    return w, b, J_history #returns final w,b and J history for graphing"
      ],
      "metadata": {
        "id": "W2DbY64BlFJY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Initialization of the gradient desent"
      ],
      "metadata": {
        "id": "bsOF8GGYo5NC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "initial_w = np.zeros_like(w_init)\n",
        "initial_b = 0.\n",
        "w,b,h = gradient_descent(X_train, y_train, initial_w, initial_b,compute_cost, compute_gradient, alpha = 1e-7,num_iters=10)\n",
        "m,n = X_train.shape\n",
        "sum=0\n",
        "for i in range(m):\n",
        "    sum = (np.dot(X_train[i], w) + b)\n",
        "    print(f\"prediction: {sum}, target value: {y_train[i]}\")    "
      ],
      "metadata": {
        "id": "XbVsh3u7lFRr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Initialization of the Normalized training set for computing the gradient desent"
      ],
      "metadata": {
        "id": "Obqrqt5po_nb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "w,b,h = gradient_descent(X_norm, y_train, initial_w, initial_b,compute_cost, compute_gradient, alpha = 1.0e-1,num_iters=1000)"
      ],
      "metadata": {
        "id": "Akm5pLvGlLzK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Final verification after the training"
      ],
      "metadata": {
        "id": "r3TleAL1pLjs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "m = X_norm.shape[0]\n",
        "yp = np.zeros(m)\n",
        "for i in range(m):\n",
        "    yp[i] = np.dot(X_norm[i], w) + b\n",
        "    print(yp[i])"
      ],
      "metadata": {
        "id": "MFDWLr_MlL53"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Prediction using the trained model"
      ],
      "metadata": {
        "id": "-tLAoTPSpXzy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x_house = np.array([1200, 3, 1, 40])\n",
        "x_house_norm = (x_house - X_mu) / X_sigma\n",
        "print(x_house_norm)\n",
        "x_house_predict = np.dot(x_house_norm, w) + b\n",
        "print(f\" predicted price of a house with 1200 sqft, 3 bedrooms, 1 floor, 40 years old = ${x_house_predict*1000}\")"
      ],
      "metadata": {
        "id": "ATJRmxFelL_L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**THE END Peace out::** \n"
      ],
      "metadata": {
        "id": "Y9hrg6_qlWvV"
      }
    }
  ]
}